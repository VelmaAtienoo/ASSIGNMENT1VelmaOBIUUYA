# -*- coding: utf-8 -*-
"""VELMA_ATIENO._SportsPrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Mt9XBwfrWPtVgp8eMdboGodh2M3VOAcW
"""

import pandas as pd

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import cross_val_score
import pickle
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

from sklearn.preprocessing import StandardScaler

from sklearn.model_selection import train_test_split

# Load a sample of the datasets to identify column names at indices 25 and 108
df = pd.read_csv('male_players (legacy).csv')
df2 = pd.read_csv('players_22.csv')

# Print column names at indices 25 and 108
legacy_column_25 = df.columns[25]
legacy_column_108 = df.columns[108]
players_22_column_25 = df2.columns[25]
players_22_column_108 = df2.columns[108]

print("Legacy Dataset Column 25:", legacy_column_25)
print("Legacy Dataset Column 108:", legacy_column_108)
print("Players 22 Dataset Column 25:", players_22_column_25)
print("Players 22 Dataset Column 108:", players_22_column_108)

# Define data types for columns with mixed types using column names
dtype_spec = {
    legacy_column_25: 'str',
    legacy_column_108: 'str',
    players_22_column_25: 'str',
    players_22_column_108: 'str'
}

#Loading the dataset given
df2 = pd.read_csv('players_22.csv',low_memory =False)#, dtype=dtype_spec) #player_22
df= pd.read_csv('male_players (legacy).csv',low_memory =False)#, dtype=dtype_spec) #male_legacy

df.head()

df2.head()

df.info()

df2.info()

# Inspect the shape of the datasets
print("Training data shape:", df.shape)
print("Test data shape:", df2.shape)

# Inspect missing values in the datasets
print("Missing values in training data:\n", df.isnull().sum())
print("Missing values in test data:\n", df2.isnull().sum())

missing_percent = df.isnull().sum() / len(df)

# Display the percentage of missing data
print(missing_percent)

# Drop columns with more than 30% missing values
threshold = 0.30
columns_to_drop = missing_percent[missing_percent > threshold].index

df = df.drop(columns=columns_to_drop)

# Display the cleaned DataFrame
df.head()

df.dtypes

#drop things that be useless
df.columns

df.drop(columns = ['player_id', 'player_url', 'fifa_version', 'fifa_update', 'fifa_update_date', 'player_face_url', 'short_name', 'long_name'], inplace = True)

df.info()

df.head()

"""# PROCESSING QUANTITATIVE DATA"""

Quantitative = df.select_dtypes(include=[np.number])
corr_matrix = Quantitative.corr()
columns_to_drop = [column for column in Quantitative.columns if abs(corr_matrix['overall'][column]) < 0.4]
Quantitative.drop(columns=columns_to_drop, axis=1, inplace=True)
corr_matrix['overall'].sort_values(ascending=False)
Quantitative.columns

important_cols = ['body_type', 'preferred_foot']
Categorical = df.select_dtypes(include = ['object'])
Categorical.drop(columns = [column for column in Categorical.columns if column not in important_cols], inplace =True)
Categorical.columns

QuantandCat = pd.concat([Categorical, Quantitative], axis = 1)
QuantandCat.columns

y = df['overall'] #our target variable

quant = df.select_dtypes(include = [np.number]) #we get all the quantitative features

corr_matrix = quant.corr()

corr_matrix['overall'].sort_values(ascending = False)

#we are dropping columns that have a correlation less than 0.4 and greater than -0.4
for column in quant.columns:
  if corr_matrix['overall'][column] < 0.4 and corr_matrix['overall'][column] > -0.4:
    quant.drop(column, axis = 1, inplace = True)

quant.info()

corr_matrix = quant.corr()

corr_matrix['overall'].sort_values(ascending = False)

#we need to impute cause there are missing values
imputer = SimpleImputer(strategy = 'median')
q_columns = quant.columns
quant = imputer.fit_transform(quant)
quant = pd.DataFrame(quant, columns = q_columns)

quant

#after imputing then we need to scale
scaler = StandardScaler()
quant = pd.DataFrame(scaler.fit_transform(quant), columns = quant.columns)
quant

quant.drop('overall', axis = 1, inplace = True)

# common relevant columns present in both datasets
relevant_columns = [col for col in Quantitative if col in df.columns and col in df2.columns]

# Extracting features and target variable from the training data
X = df[relevant_columns]
Y = df['overall']

"""# PROCESSING CATEGORICAL DATA"""

cat = df.select_dtypes(include = ['object'])
cat.info()

important_cols = ['preferred_foot', 'body_type']
cat.drop(columns = [column for column in cat.columns if column not in important_cols], inplace =True)

cat.info()

#one hot encoding
cat = pd.get_dummies(cat).astype(int)
cat

"""# USING ALL OUR DATA TO TRAIN AND TEST THE MODEL"""

# Handle categorical features using one-hot encoding
X = pd.get_dummies(X, drop_first=True)

# Fill missing values with the mean for numerical features
X.fillna(X.mean(), inplace=True)

# Standardize the features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Split the training data into training and validation sets
X, X_val, Y, y_val = train_test_split(X, Y, test_size=0.2, random_state=42)

from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
import numpy as np

# Initialize the ML models
models = {
    'Linear Regression': LinearRegression(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor()
}

linearmod=LinearRegression()
DeciderTree = DecisionTreeRegressor()
GradBoost = GradientBoostingRegressor()
RandFor = RandomForestRegressor()

# Evaluate models using cross-validation
results = {}
for name, model in models.items():
    scores = cross_val_score(model, X, Y, cv=5, scoring='neg_mean_squared_error')
    results[name] = np.sqrt(-scores.mean())

print("Model Evaluation Results:", results)

with open('linear_model.pkl', 'wb') as file:
    pickle.dump(linearmod, file)  # Save the linear regression model

with open('GradBoost.pkl', 'wb') as file:
    pickle.dump(GradBoost, file)  # Save the regression model

with open('RandFor.pkl', 'wb') as file:
    pickle.dump(RandFor, file)  # Save the regression model

with open('DeciderTree.pkl', 'wb') as file:
    pickle.dump(DeciderTree, file)  # Save the regression model

params = {
    'n_estimators': [100,200,300],
    'max_depth': [5,7,9]
}

rando_search = RandomizedSearchCV(
    estimator=model,
    param_distributions=params,
    n_iter=9,  # Reduce number of iterations
    cv=2,  # Reduce number of cross-validation folds
    n_jobs=-1,  # Use all available cores
    random_state=42
)

rando_search.fit(X, Y)

best_model = rando_search.best_estimator_
best_model

with open('best_model.pkl', 'wb') as file:
    pickle.dump(best_model, file)  # Save the regression model

# Train the best model on the full training data and evaluate on the validation set
best_model.fit(X, Y)
y_pred = best_model.predict(X_val)
print("Validation RMSE:", np.sqrt(mean_squared_error(y_val, y_pred)))
best_model

# Prepare the test data in a similar way
X_test = df2[relevant_columns]
X_test = pd.get_dummies(X_test, drop_first=True)
X_test.fillna(X_test.mean(), inplace=True)
X_test = scaler.transform(X_test)

# Predic the overall ratings for the test data
df2['predicted_overall'] = best_model.predict(X_test)

# Save the predictions
df2.to_csv('predicted_players_22.csv', index=False)
print("Predictions saved to predicted_players_22.csv")

#X = pd.concat([cat, quant], axis = 1)ta

#Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, y, test_size = 0.2, random_state = 42)

#from sklearn.linear_model import LinearRegression
#from sklearn.tree import DecisionTreeRegressor
#from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

#lin_reg = LinearRegression()

#lin_reg.fit(Xtrain, Ytrain) #da training function
#lin_reg_pred = lin_reg.predict(Xtest)

#print(f""" Mean Aboslute Error={mean_absolute_error(lin_reg_pred,Ytest)}
          #Mean Squared Error = {mean_squared_error(lin_reg_pred,Ytest)}
          #Root Mean Squared error = {np.sqrt(mean_squared_error(lin_reg_pred,Ytest))}
          #R2 score = {r2_score(lin_reg_pred,Ytest)}
#""")

#dTree = DecisionTreeRegressor()
#dTree.fit(Xtrain, Ytrain)

#dTree_pred = dTree.predict(Xtest)

#print(f""" Mean Aboslute Error={mean_absolute_error(dTree_pred,Ytest)}
          #Mean Squared Error = {mean_squared_error(dTree_pred,Ytest)}
          #Root Mean Squared error = {np.sqrt(mean_squared_error(dTree_pred,Ytest))}
          #R2 score = {r2_score(dTree_pred,Ytest)}
#""")

#from sklearn.ensemble import RandomForestRegressor
#from sklearn.model_selection import RandomizedSearchCV

#model = RandomForestRegressor()

#parameter = {
    #'n_estimators': [100,200,300],
    #'max_depth': [5,7,9]
#}

#rand_search = RandomizedSearchCV(model, parameter, scoring = 'r2', cv = 3)

#rand_search.fit(Xtrain, Ytrain)

# pickling the model
#import pickle
#pickle_out = open("classifier.pkl", "wb")
#pickle.dump(classifier, pickle_out)
#pickle_out.close()

#!pip install streamlit









